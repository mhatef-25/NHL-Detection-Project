import tensorflow as tf
from tensorflow.keras.applications import VGG16
from tensorflow.keras.preprocessing.image import ImageDataGenerator
from tensorflow.keras.layers import Dense, GlobalAveragePooling2D, Dropout
from tensorflow.keras.models import Model
from tensorflow.keras.optimizers import AdamW
from google.colab import drive
drive.mount('/content/gdrive')
img_height, img_width = 640, 640
num_classes = 2  # Adjust based on the number of classes in your dataset
train_datagen = ImageDataGenerator(
    rescale=1./255,               
    validation_split=0.18,         
    vertical_flip=True,           
    horizontal_flip=True,         
    brightness_range=[0.6, 1.4],  
    shear_range=0.1,              
    zoom_range=[0.7, 1.3],        
    width_shift_range=0.1,        
    height_shift_range=0.1,      
    fill_mode='nearest'           
)
val_datagen = ImageDataGenerator(
    rescale=1./255,               
    validation_split=0.18         
)
test_datagen = ImageDataGenerator(
    rescale=1./255                
)
train_generator = train_datagen.flow_from_directory(
    '/content/gdrive/My Drive/dataset/train',
    target_size=(img_height, img_width),
    batch_size=16,
    class_mode='categorical',
    subset='training'
)

validation_generator = val_datagen.flow_from_directory(
    '/content/gdrive/My Drive/dataset/train',
    target_size=(img_height, img_width),
    batch_size=16,
    class_mode='categorical',
    subset='validation'
)
test_generator = test_datagen.flow_from_directory(
    '/content/gdrive/My Drive/dataset/test', 
    target_size=(img_height, img_width),
    batch_size=16,
    class_mode='categorical'
)
base_model = VGG16(weights='imagenet', include_top=False, input_shape=(img_height, img_width, 3))
x = base_model.output
x = GlobalAveragePooling2D()(x)
x = Dense(1024, activation='relu')(x)
x = Dropout(0.5)(x)
predictions = Dense(num_classes, activation='softmax')(x)
model = Model(inputs=base_model.input, outputs=predictions)
for layer in base_model.layers:
    layer.trainable = False
model.compile(
    optimizer=AdamW(learning_rate=0.000714, weight_decay=0.01),
    loss='categorical_crossentropy',
    metrics=['accuracy']
)
history = model.fit(
    train_generator,
    steps_per_epoch=train_generator.samples // train_generator.batch_size,
    validation_data=validation_generator,
    validation_steps=validation_generator.samples // validation_generator.batch_size,
    epochs=100  # Adjust epochs based on your needs
)
val_loss, val_accuracy = model.evaluate(validation_generator)
print(f'Validation Loss: {val_loss}')
print(f'Validation Accuracy: {val_accuracy}')
test_loss, test_accuracy = model.evaluate(test_generator)
print(f'Test Loss: {test_loss}')
print(f'Test Accuracy: {test_accuracy}')
model_save_path = '/content/gdrive/My Drive/dataset/vgg16_model.h5'
model.save(model_save_path)
print(f'Model saved to: {model_save_path}')
